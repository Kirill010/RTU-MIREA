{"nbformat":4,"nbformat_minor":0,"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"position":{"height":"185.25px","left":"381.8px","right":"20px","top":"119px","width":"324.4px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ksOIFy4GZ4Yf"},"source":["# Нейронные сети, глубокое обучение. Фреймворки.\n"]},{"cell_type":"markdown","metadata":{"id":"9mo1o09vZ4Ym"},"source":["## Глубокое обучение\n","Модели, с которыми мы с вами успели познакомиться довольно простые, существуют гораздо более сложные модели, которые занимают много места, считаются долго, но зато дают хорошие результаты.\n","\n","На примере бустинга и стемминга мы видели, что модели можно соединять друг с другом, и тут возникла идея, а что если соединять между собой простые модельки, но сделать связь между ними более сложной, не даст ли это хорошего результата.\n","\n","Так появились нейронные сети, сети из простых соединенных между собой моделей (их называют нейронами), в которых соединения имеют настраиваемые веса - изменяя вес связи нейронов мы можем получать нужную сложность сети в целом. В зависимости от того, как именно соединены нейроны, получаем разные типы нейронных сетей. Нейроны могут располагаться в слоях, когда они работают одновременно (параллельно), или могут быть соединены последовательно, тогда работа следующего зависит от предыдущего, или вообще любым другим способом. Когда в нейронной сети много слоев нейронов, то такую модель называют \"глубокой\" (иногда \"глубинной\", англ. deep), а обучение таких глубоких моделей - **глубоким обучением** (deep learning).\n"]},{"cell_type":"markdown","metadata":{"id":"pRjmbdclZ4Yq"},"source":["## Графические ускорители\n","Глубокие модели огромны, содержат  тысячи, сотни тысяч и миллионы параметров, которые нужно обучать. Это требует мощных компьютеров. Все настолько серьезно, что обычные компьютеры, которыми вы пользуетесь, слабо подходят для глубокого обучения.\n","\n","А как сделать компьютер быстрее? Есть два варианта:\n","- повышать тактовую частоту процессора, которая показывает как часто происходят вычисления. Сегодня привычны процессоры с 3 гигагерцами тактовой частоты, что значит что за одну секунду может произойти 3 *миллиарда* переключений (вычислений). Долгое время компьютеры развивались именно повышением тактовой частоты, но всему есть предел. Теперь повышать тактовую частоту все сложней и сложней, и конечно дороже, это связано с физикой, из за того, что изменения тока в одном проводнике создают электромагнитные колебания, которые просачиваются в соседние проводники, и никуда от этого не денешься. Поэтому сейчас повысить, скажем, в два раза тактовую частоту будет стоить огромных денег, но ускорять компьютер необходимо.\n","- поэтому есть другой путь для ускорения - делать вычисления одновременно (параллельно) несколькими процессорами и как-то подгадать, чтобы наши алгоритмы могли работать параллельно. Тактовая частота не изменится, но за один такт, имея несколько процессоров, мы можем произвести больше вычислений. И именно по этому пути сегодня развивается вычислительная техника.\n","\n","Но на пути параллельных вычислений есть свои трудности. Если ваш алгоритм последователен - для следующего действия требуется результат предыдущего - то его никак не выполнить параллельно, один процессор должен ждать, пока не закончит другой. Нужны другие алгоритмы. Оказалось, что и нейронные сети и многие алгоритмы машинного обучения параллельны, значит хорошо подходят для использования на параллельной вычислительной технике.\n","\n","Сколько процессоров (микропроцессоров, central processing unit, CPU) может быть в обычном компьютере? Два? Четыре? Восемь? Не больше. Значит можно распараллелить вычисления всего-то в восемь раз. Сегодня этого мало. Нужны другие процессоры. Но вот удивительно, в обычном современном компьютере есть такое место, где этих процессоров может быть тысячи. Это **графические ускорители** (графическая плата, видеоплата, graphical processing unit, GPU). Оказывается те графические ускорители, которые вы используете для игрушек, на самом-то деле являются примером параллельных вычислителей, и могут выполнять сложные вычисления.\n","\n","Особенно отличилась компания NVIDIA которая теперь создает GPU не только для игрушек но и для серьезных расчетов. Именно на таких GPU обучаются большие модели. Почему же? В чем причина такой популярности GPU?\n","\n","Давайте посмотрим на картинку со схемой типичного CPU и GPU.\n","\n","![img](https://drive.google.com/uc?id=1Yv-8ed37V7LDeU3z9TGEOCNT_yeWIVhh)\n","\n","У CPU есть несколько (2-8) арифметико-логических устройств (ALU, зеленые) которые выполняют вычисления и могут работать параллельно, есть довольно большая и быстрая память кэша (cache) которая доступна всем ALU и которая используется для подгрузки нужных данных и память DRAM помедленнее, которую используют для хранения данных и сложное устройство управления (control), которое управляет всеми этими устройствами по отдельности.\n","\n","У GPU же есть много (сотни) ALU, но они гораздо более простые, работают на меньшей частоте, памяти DRAM существенно меньше (но возможно она быстрее), и устройство управления попроще, поскольку здесь нельзя управлять каждым ALU по отдельности, а только группой связанных ALU. Такая архитектура с одной стороны очень подходит для параллельных вычислений, но с другой сильно ограничивает возможности. К примеру, если в группе у вас 32 ALU, а нужно сложить только два числа, то все остальные все равно будут заниматься сложением чисел, хоть это вам и не надо, т.е. выполнять лишнюю бесполезную работу - это из-за того, что можно управлять группой целиком, а не отдельным ALU. **Не все можно эффективно распараллелить!**.\n","\n","Программирование параллельных вычислителей гораздо сложней, но к счастью, сегодня есть средства, которые делают работу за нас."]},{"cell_type":"markdown","source":["## Дифференцирование\n","Для примера, пусть нам потребовалось найти производную следующей функции:"],"metadata":{"id":"RMLFWDFF6FOi"}},{"cell_type":"markdown","source":["$ z = x * y \\\\\n","c = x * x \\\\\n","d = y - z \\\\\n","e = c ^ d $\n","\n","$ \\frac{\\partial e}{\\partial x} = ??? $"],"metadata":{"id":"EaLGSsCTzbd6"}},{"cell_type":"markdown","source":["### Символьные вычисления и дифференцирование\n","Мы могли бы воспользоваться *символьными* вычислениями и заставить компьютер найти такую производую.\n","Для символьных вычислений подходит библиотека [`sympy`](https://docs.sympy.org/latest/index.html)."],"metadata":{"id":"HpYNIDog5_eR"}},{"cell_type":"code","source":["import sympy # подключим библиотеку для символьных вычислений\n","x_s=sympy.symbols('x') # переменные объявим как \"символы\"\n","y_s=sympy.symbols('y') #\n","e_s=(x_s*x_s)**(y_s-x_s*y_s) # выражение с символьными переменными\n","res=sympy.diff(e_s,x_s) # находим производную\n","print('Символьное выражение e(x,y)=',res) # она представлена также в символьном виде\n","print('Численный результат  e(2,3)=',res.subs([(x_s,2.0),(y_s,3.0)])) # подставим значения вместо символов"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1falpXRj2f1U","executionInfo":{"status":"ok","timestamp":1701098122869,"user_tz":-180,"elapsed":1505,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"d2351888-2f59-4a05-bad1-4529eaeb61b7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Символьное выражение e(x,y)= (-y*log(x**2) + 2*(-x*y + y)/x)*(x**2)**(-x*y + y)\n","Численный результат  e(2,3)= -0.111857548177495\n"]}]},{"cell_type":"markdown","source":["Символьные вычисления хороши только для простых вычислений, чем сложней выражение, тем сложней будут символьные вычисления. Более того, не все можно представить в элементарных функциях.\n","\n","Тогда на помощь приходит автодифференцирование.\n"],"metadata":{"id":"DfnWwFY19vdF"}},{"cell_type":"markdown","metadata":{"id":"QlK9NvbXZ4Yt"},"source":["## Фреймворки глубокого обучения и Автодифференцирование\n","Созданы **фреймворки глубокого обучения** специальные библиотеки программ, которые позволяют работать с нейронными сетями (и другими алгоритмами машинного обучения) и уже из коробки могут задействовать всю мощь GPU. Познакомимся кратко с некоторыми из них.\n","\n","\n","Основой любого фреймворка глубокого обучения является **автодифференцирование**. Мы уже упоминали, что самым популярным методом обучения является градиентный спуск и его модификации, для чего необходимо считать производные функции ошибки по всем настраиваемым параметрам. Вспомним школьное правило дифференцирования сложных (составных) функций: пусть, например, функция А зависит от функций В и С, которые зависят от аргумента х, тогда производная A по x получается как: \\\\(  A(B(x),C(x)) ==> \\frac{\\partial A}{\\partial x} = \\frac{\\partial A}{\\partial B} * \\frac{\\partial B}{\\partial x} + \\frac{\\partial A}{\\partial C} * \\frac{\\partial C}{\\partial x}\\\\)\n","\n","Значит для любой дифференцируемой функции мы можем найти производную, составив дерево зависимостей функций друг от друга и применяя такие правила. Даже если наши \"функции\" это программы, которые принимают некоторые аргументы, возвращают некоторые значения. Например программа А - складывает два аргумента, В - умножает входной аргумент на 3, С - возводит входной аргумент в квадрат, тогда получим:\n","\n","$$ A=B+C ==> \\frac{\\partial A}{\\partial B} = 1,   \\frac{\\partial A}{\\partial C} = 1, \\\\\n","B=3*x ==> \\frac{\\partial B}{\\partial x} = 3 \\\\\n","C=x*x ==> \\frac{\\partial C}{\\partial x} = 2 * x \\\\\n","и\\ окончательно\\ \\frac{\\partial A}{\\partial x} = 1 * 3+1 * (2 * x) $$\n","\n","Если мы знаем производные выхода программы по ее входам, то можем найти такие производные и для других программ, которые вызывают первую. И конечно это можно делать автоматически. При вызове программ нужно записывать дерево вычислений - какие программы друг друга вызывали, а когда потребуется вычислить производную - посмотреть на это дерево вычислений и применить правила для расчета производных составных функций. Если все действия в программе дифференцируемы (или могут быть принудительно сделаны дифференцируемыми), то можем найти и производную по любому аргументу этой программы. Это и есть автодифференцирование.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P6ZmlRDTZ4Yv"},"source":["В фреймворках модели представляют собой некоторые объекты, которые поддерживают автодифференцирование. И тогда создавая модель, например добавляя в нее слои, функции, входы и пр. мы можем потом искать и производные и тем самым обучать модель.\n","\n","Один из самых известных фреймворков глубокого обучения [TensorFlow](https://www.tensorflow.org/tutorials/quickstart/beginner) и его надстройка [Keras](https://keras.io/getting_started/). Вот как выглядит пример реализации модели на нем:\n","\n","``\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(10)\n","])\n","``\n","\n","Создается последовательная модель  `Sequential` в которую постепенно добавляются слои нейронов или функций, выход одного слоя является входом  следующего, а это и есть дерево связей, здесь оно последовательное, без разветвлений. Конечно для моделей реализованы методы для обучения разными способами, расчета выходов и др., они могут называться по-другому чем в `sklearn` но смысл их такой же. Например метод `.fit()` что делает? Правильно, обучает модель на заданных примерах, а метод `.evaluate()`? Можно догадаться по названию (а лучше конечно смотреть в документации), что это оценка работы метода для заданных примеров (в `sklearn` в точности такого же метода нет, но похоже на `.score()`).\n","\n"]},{"cell_type":"markdown","source":["#### `torch.autograd.backward()`\n","Другой известный фреймворк [PyTorch](https://pytorch.org/tutorials/) тоже строит граф вычислений и позволяет посмотреть на него (функция `make_dot` из библиотеки `torchviz`).\n","\n","Посмотрим на пример.\n"],"metadata":{"id":"Rhyhfs_4QbaS"}},{"cell_type":"markdown","source":["Для того, чтобы воспользоваться автодифференцированием в torch мы должны объявить переменные специальным типом \"тензор\" (`tensor`) и задать свойство `requires_grad`, чтобы сообщить, что мы будем накапливать производные по этим переменным.\n","\n","Тензоры - многомерные массивы - можно использовать в вычислениях, и граф этих вычислений будет накапливаться."],"metadata":{"id":"7JzPbtG9AUqF"}},{"cell_type":"code","metadata":{"id":"TMr28HTAbhqf","executionInfo":{"status":"ok","timestamp":1701098329512,"user_tz":-180,"elapsed":10891,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}}},"source":["%%capture\n","!pip install torchviz"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-11-05T09:38:39.871947Z","start_time":"2020-11-05T09:38:39.746939Z"},"id":"xiUSYNpPZ4Yx","executionInfo":{"status":"ok","timestamp":1701098910532,"user_tz":-180,"elapsed":245,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}}},"source":["import torch # подключаем torch\n","\n","# задаем вычисления\n","x = torch.tensor(2.0, requires_grad = True) # исходная переменная х\n","y = torch.tensor(3.0,requires_grad = True) # исходная переменная у\n","\n","#какие-то вычисления:\n","z = x * y # перемножаем\n","c=x*x # квадрат\n","d=y-z # вычитание\n","e=c**d # степень\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from torchviz import make_dot # для просмотра графа вычислений\n","params=dict(x=x,y=y) # названия переменных\n","make_dot(e,params=params)  # строим граф"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"id":"JqQ1rSBu4z6t","executionInfo":{"status":"ok","timestamp":1701098524562,"user_tz":-180,"elapsed":986,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"cae36d3c-2a91-4b76-92fb-e13b019ff143"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"258pt\" height=\"325pt\"\n viewBox=\"0.00 0.00 258.00 325.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 321)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-321 254,-321 254,4 -4,4\"/>\n<!-- 133504795195840 -->\n<g id=\"node1\" class=\"node\">\n<title>133504795195840</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"155.5,-31 101.5,-31 101.5,0 155.5,0 155.5,-31\"/>\n<text text-anchor=\"middle\" x=\"128.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 133504795773440 -->\n<g id=\"node2\" class=\"node\">\n<title>133504795773440</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"173,-86 84,-86 84,-67 173,-67 173,-86\"/>\n<text text-anchor=\"middle\" x=\"128.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">PowBackward1</text>\n</g>\n<!-- 133504795773440&#45;&gt;133504795195840 -->\n<g id=\"edge11\" class=\"edge\">\n<title>133504795773440&#45;&gt;133504795195840</title>\n<path fill=\"none\" stroke=\"black\" d=\"M128.5,-66.79C128.5,-60.07 128.5,-50.4 128.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"132,-41.19 128.5,-31.19 125,-41.19 132,-41.19\"/>\n</g>\n<!-- 133504795773056 -->\n<g id=\"node3\" class=\"node\">\n<title>133504795773056</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-196 155,-196 155,-177 244,-177 244,-196\"/>\n<text text-anchor=\"middle\" x=\"199.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 133504795773056&#45;&gt;133504795773440 -->\n<g id=\"edge1\" class=\"edge\">\n<title>133504795773056&#45;&gt;133504795773440</title>\n<path fill=\"none\" stroke=\"black\" d=\"M193.69,-176.66C181.93,-158.77 155.13,-118 139.8,-94.69\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"142.61,-92.59 134.19,-86.16 136.76,-96.44 142.61,-92.59\"/>\n</g>\n<!-- 133504795773392 -->\n<g id=\"node4\" class=\"node\">\n<title>133504795773392</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"250,-251 149,-251 149,-232 250,-232 250,-251\"/>\n<text text-anchor=\"middle\" x=\"199.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 133504795773392&#45;&gt;133504795773056 -->\n<g id=\"edge2\" class=\"edge\">\n<title>133504795773392&#45;&gt;133504795773056</title>\n<path fill=\"none\" stroke=\"black\" d=\"M194.33,-231.75C192.84,-224.8 192.4,-214.85 193.02,-206.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"196.51,-206.47 194.37,-196.09 189.57,-205.53 196.51,-206.47\"/>\n</g>\n<!-- 133504795773392&#45;&gt;133504795773056 -->\n<g id=\"edge4\" class=\"edge\">\n<title>133504795773392&#45;&gt;133504795773056</title>\n<path fill=\"none\" stroke=\"black\" d=\"M204.67,-231.75C206.16,-224.8 206.6,-214.85 205.98,-206.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"209.43,-205.53 204.63,-196.09 202.49,-206.47 209.43,-205.53\"/>\n</g>\n<!-- 133504795772960 -->\n<g id=\"node9\" class=\"node\">\n<title>133504795772960</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"137,-196 48,-196 48,-177 137,-177 137,-196\"/>\n<text text-anchor=\"middle\" x=\"92.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 133504795773392&#45;&gt;133504795772960 -->\n<g id=\"edge9\" class=\"edge\">\n<title>133504795773392&#45;&gt;133504795772960</title>\n<path fill=\"none\" stroke=\"black\" d=\"M182.31,-231.98C165.25,-223.54 138.87,-210.47 119.1,-200.68\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"120.52,-197.47 110,-196.17 117.41,-203.75 120.52,-197.47\"/>\n</g>\n<!-- 133504795132928 -->\n<g id=\"node5\" class=\"node\">\n<title>133504795132928</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"226.5,-317 172.5,-317 172.5,-287 226.5,-287 226.5,-317\"/>\n<text text-anchor=\"middle\" x=\"199.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\">x</text>\n<text text-anchor=\"middle\" x=\"199.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 133504795132928&#45;&gt;133504795773392 -->\n<g id=\"edge3\" class=\"edge\">\n<title>133504795132928&#45;&gt;133504795773392</title>\n<path fill=\"none\" stroke=\"black\" d=\"M199.5,-286.84C199.5,-279.21 199.5,-269.7 199.5,-261.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"203,-261.27 199.5,-251.27 196,-261.27 203,-261.27\"/>\n</g>\n<!-- 133504795773152 -->\n<g id=\"node6\" class=\"node\">\n<title>133504795773152</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"137,-141 48,-141 48,-122 137,-122 137,-141\"/>\n<text text-anchor=\"middle\" x=\"92.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">SubBackward0</text>\n</g>\n<!-- 133504795773152&#45;&gt;133504795773440 -->\n<g id=\"edge5\" class=\"edge\">\n<title>133504795773152&#45;&gt;133504795773440</title>\n<path fill=\"none\" stroke=\"black\" d=\"M98.44,-121.75C103.48,-114.34 110.84,-103.5 117.01,-94.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"119.94,-96.33 122.67,-86.09 114.15,-92.39 119.94,-96.33\"/>\n</g>\n<!-- 133504795773296 -->\n<g id=\"node7\" class=\"node\">\n<title>133504795773296</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-251 0,-251 0,-232 101,-232 101,-251\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 133504795773296&#45;&gt;133504795773152 -->\n<g id=\"edge6\" class=\"edge\">\n<title>133504795773296&#45;&gt;133504795773152</title>\n<path fill=\"none\" stroke=\"black\" d=\"M45.68,-231.86C39.32,-219.24 29.88,-195.26 38.5,-177 44.51,-164.26 56.16,-153.92 67.11,-146.44\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"69.04,-149.36 75.61,-141.05 65.29,-143.44 69.04,-149.36\"/>\n</g>\n<!-- 133504795773296&#45;&gt;133504795772960 -->\n<g id=\"edge10\" class=\"edge\">\n<title>133504795773296&#45;&gt;133504795772960</title>\n<path fill=\"none\" stroke=\"black\" d=\"M57.44,-231.75C63.37,-224.26 72.07,-213.28 79.32,-204.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"82.23,-206.1 85.69,-196.09 76.74,-201.75 82.23,-206.1\"/>\n</g>\n<!-- 133508180183696 -->\n<g id=\"node8\" class=\"node\">\n<title>133508180183696</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-317 23.5,-317 23.5,-287 77.5,-287 77.5,-317\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\">y</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 133508180183696&#45;&gt;133504795773296 -->\n<g id=\"edge7\" class=\"edge\">\n<title>133508180183696&#45;&gt;133504795773296</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-286.84C50.5,-279.21 50.5,-269.7 50.5,-261.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-261.27 50.5,-251.27 47,-261.27 54,-261.27\"/>\n</g>\n<!-- 133504795772960&#45;&gt;133504795773152 -->\n<g id=\"edge8\" class=\"edge\">\n<title>133504795772960&#45;&gt;133504795773152</title>\n<path fill=\"none\" stroke=\"black\" d=\"M92.5,-176.75C92.5,-169.8 92.5,-159.85 92.5,-151.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"96,-151.09 92.5,-141.09 89,-151.09 96,-151.09\"/>\n</g>\n</g>\n</svg>\n","text/plain":["<graphviz.graphs.Digraph at 0x796c01ed8a30>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"VBUgtLCWZ4Y1"},"source":["Отображаются исходные переменные и действия, которые с ними проводились. По названию можно догадаться что за действия: Mul - умножение, Sub - вычитание, Pow - степень.\n","\n","Посмотрите на картинку и объясните каждую стрелку, найдите и укажите место где получаются переменные z, c, d, e.\n","\n","Сделайте аналогично для своих примеров вычислений."]},{"cell_type":"markdown","source":["Теперь у тензора `e`, полученного из исходных `x и y` есть граф вычислений и при необходимости посчитать производную тензора такой граф будет использован.\n","Модуль [`torch.autograd`](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) позволяет работать с автодифференцированием.\n","\n","Вызываем метод `backward()`, который будет рассчитывать (накапливать) производные по всем изначальным аргументам (это листья x и y)."],"metadata":{"id":"YVD5djmT1LVp"}},{"cell_type":"code","source":["e.backward()"],"metadata":{"id":"S3kcwxV_w34q","executionInfo":{"status":"ok","timestamp":1701098915466,"user_tz":-180,"elapsed":226,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Посмотрим на найденную производную для `x`, которая хранится в поле `.grad`:"],"metadata":{"id":"8UJQgbBcCGXI"}},{"cell_type":"code","source":["x.grad"],"metadata":{"id":"BqdYLZWRyiGc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701098798998,"user_tz":-180,"elapsed":256,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"d067e35e-a35d-4992-aa34-361d2d5b3eb6"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.1119)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["y.grad"],"metadata":{"id":"Ca-BfMxrro5r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701098801206,"user_tz":-180,"elapsed":258,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"f0681e31-7f27-4c0d-ec4e-c32f9ecd2432"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.0217)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["d.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VhM-j3ZcrrIp","executionInfo":{"status":"ok","timestamp":1701098957673,"user_tz":-180,"elapsed":224,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"051780e3-d0cb-4183-8bc1-6620a2c06938"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-e81247674995>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n","  d.grad\n"]}]},{"cell_type":"markdown","source":["Совпадает с расчетом символьным способом?"],"metadata":{"id":"0uqpbcpoCf-s"}},{"cell_type":"markdown","source":["В случае, если переменная, от которой вы ищите производную не скаляр, то непонятно, какую именно производную нужно вернуть. Будет искаться, на самом деле, произведение матрицы Якоби на некий вектор (вектор производных некой скалярной функции)), для этого надо такой вектор задать в аргументе `gradient`.\n","\n","Если такой вектор состоит из единиц, тогда это эквивалентно сумме производных по всем компонентам требуемого массива, что обычно и требуется. Альтернатива - сначала просуммировать не скалярный массив, потом уже искать производную.\n"],"metadata":{"id":"Y2XEj0weDnS5"}},{"cell_type":"code","source":["import torch\n","\n","a = torch.tensor([2., 3.], requires_grad=True) # аргумент\n","b = torch.tensor([6., 4.], requires_grad=True) # аргумент\n","Q = 3*a**3 - b**2 # выражение\n"],"metadata":{"id":"03LjDnoDCvpG","executionInfo":{"status":"ok","timestamp":1701099291560,"user_tz":-180,"elapsed":243,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOFQRi-_4Mdx","executionInfo":{"status":"ok","timestamp":1701099307475,"user_tz":-180,"elapsed":246,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}}},"source":["external_grad = torch.tensor([1., 1.]) # \"внешний градиент\"\n","Q.backward(gradient=external_grad)\n","#Q.backward() # не работает для не скаляров\n","#Q.sum().backward() # альтернатива \"внешнему\" градиенту"],"execution_count":17,"outputs":[]},{"cell_type":"code","source":["print(9*a**2,  a.grad)\n","print(-2*b, b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ue3wVX4DHLK","executionInfo":{"status":"ok","timestamp":1701099341400,"user_tz":-180,"elapsed":243,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"20d10e46-7143-41d3-a65a-3c251a8ae70a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([36., 81.], grad_fn=<MulBackward0>) tensor([36., 81.])\n","tensor([-12.,  -8.], grad_fn=<MulBackward0>) tensor([-12.,  -8.])\n"]}]},{"cell_type":"markdown","source":["\n","Метод `backward()` применяется только один раз, и сбрасывает граф вычислений. Если потом понадобится искать производную еще раз (но зачем??), надо сохранять граф с помощью аргумента `retain_graph`."],"metadata":{"id":"oG7z57oiGqGk"}},{"cell_type":"code","source":["a = torch.tensor([2., 3.], requires_grad=True) # аргумент\n","b = torch.tensor([6., 4.], requires_grad=True) # аргумент\n","Q = 3*a**3 - b**2 # выражение\n","#Q.sum().backward()\n","Q.sum().backward(retain_graph=True)"],"metadata":{"id":"pkqn1Kb0HVhM","executionInfo":{"status":"ok","timestamp":1701099382739,"user_tz":-180,"elapsed":235,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# граф сохранился\n","a.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQgY0ebUH_n0","executionInfo":{"status":"ok","timestamp":1701099385160,"user_tz":-180,"elapsed":263,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"89a8557d-5f2d-4d82-b85e-b58cbecd55a1"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([36., 81.])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["Q.sum().backward(retain_graph=True)"],"metadata":{"id":"bIkRbsSXI2k0","executionInfo":{"status":"ok","timestamp":1701099389139,"user_tz":-180,"elapsed":275,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["a.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddXQIALsI5q-","executionInfo":{"status":"ok","timestamp":1701099391338,"user_tz":-180,"elapsed":239,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"ba3bfd15-45cd-4729-a5ee-bd6fa3b77f91"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 72., 162.])"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["# `torch.autograd.grad()`\n","`backward()` накапливает производные.\n","\n","Если нужно только вернуть производные, можно воспользоваться [`grad()`](https://pytorch.org/docs/stable/generated/torch.autograd.grad.html), указывая переменную от которой и по которым ищем производные."],"metadata":{"id":"XOJI8tf-KUmn"}},{"cell_type":"code","source":["a = torch.tensor([1.], requires_grad=True) # аргумент\n","Q = a**2 - 2*a + 5 # выражение\n","a_grad=torch.autograd.grad(Q,a,create_graph=True) # первая производная, создаем и сохраняем граф для нее\n","print(a_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"un7ovGJJLTl_","executionInfo":{"status":"ok","timestamp":1701099471310,"user_tz":-180,"elapsed":647,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"acbee614-1cb2-4c48-e225-fd7b00f06796"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([0.], grad_fn=<AddBackward0>),)\n"]}]},{"cell_type":"code","source":["a_grad_grad=torch.autograd.grad(a_grad,a) # вторая производная\n","print(a_grad_grad)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NTxivY-zLuLt","executionInfo":{"status":"ok","timestamp":1701099566609,"user_tz":-180,"elapsed":243,"user":{"displayName":"guntar vladovlanis","userId":"09797821821165555567"}},"outputId":"ca77c4b3-6e72-4054-8e9b-ea64579162bf"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([2.]),)\n"]}]},{"cell_type":"markdown","metadata":{"id":"jKTHlpVVZ4Y2"},"source":["## Библиотека PuzzleLib\n","Вообще всяких разных фреймворков глубокого обучения сделано уже много, см., например, [здесь](https://towardsdatascience.com/top-10-best-deep-learning-frameworks-in-2019-5ccb90ea6de).\n","\n","Россия не отстает, и наши компании тоже делают похожие фреймворки, это библиотека [PuzzleLib](https://puzzlelib.org/). Она создана совсем недавно, и разнообразие функций не так велико, как хотелось бы, но библиотека развивается, и на некоторых тестах даже превосходит известные фреймворки. Что особенно приятно - библиотека имеет документацию на русском языке. Разработчики библиотеки очень серьезно относятся к удобству именно русскоязычных пользователей, собственно данный курс и создан с этой целью.   "]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-07-28T10:45:28.440786Z","start_time":"2020-07-28T10:45:28.422785Z"},"id":"A3GDXXS0Z4Y3"},"source":["Библитотека состоит из нескольких разделов, содержащих необходимые инстурменты для создания и обучения своей нейронной сети.\n","\n","- [Modules](https://puzzlelib.org/documentation/base/modules/Modules/): Основные блоки искусственных нейронных сетей, разные типы слоев нейронов, функции преобразования.\n","- [Containers](https://puzzlelib.org/documentation/base/containers/Containers/): инструменты для соединения модулей,  установки связей между слоями нейронной сети. Позволяет соединять слои параллельно, последовательно и в виде заданного графа.\n","- [Cost](https://puzzlelib.org/documentation/base/cost/Costs/): различные функции ошибки.\n","- [Optimizers](https://puzzlelib.org/documentation/base/optimizers/Optimizers/): методы оптимизации функции ошибки, основанные на градиентном спуске.\n","- [Handlers](https://puzzlelib.org/documentation/base/handlers/Handlers/): вспомогательные инструменты для облегчения обучения, расчета и проверки нейронных сетей.\n","- [Transformers](https://puzzlelib.org/documentation/base/transformers/Transformers/): инструменты для преобразования обучающих данных налету, в процессе обучения.\n"]},{"cell_type":"code","metadata":{"id":"5_YWmoFXZ4Y6"},"source":[],"execution_count":null,"outputs":[]}]}